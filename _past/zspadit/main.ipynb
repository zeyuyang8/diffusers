{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/zy45/anaconda3/envs/diffusers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from huggingface_hub.utils import insecure_hashlib\n",
    "from peft import LoraConfig, set_peft_model_state_dict\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from PIL import Image\n",
    "from PIL.ImageOps import exif_transpose\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTokenizer, PretrainedConfig, T5TokenizerFast\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    SD3Transformer2DModel,\n",
    "    StableDiffusion3Pipeline,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import (\n",
    "    _set_state_dict_into_text_encoder,\n",
    "    cast_training_params,\n",
    "    compute_density_for_timestep_sampling,\n",
    "    compute_loss_weighting_for_sd3,\n",
    "    free_memory,\n",
    ")\n",
    "from diffusers.utils import (\n",
    "    check_min_version,\n",
    "    convert_unet_state_dict_to_peft,\n",
    "    is_wandb_available,\n",
    ")\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-3.5-medium', 'revision': None, 'variant': None, 'dataset_name': None, 'dataset_config_name': None, 'instance_data_dir': 'dog', 'cache_dir': None, 'image_column': 'image', 'caption_column': None, 'repeats': 1, 'class_data_dir': None, 'instance_prompt': 'a photo of sks dog', 'class_prompt': None, 'max_sequence_length': 77, 'validation_prompt': 'A photo of sks dog in a bucket', 'num_validation_images': 4, 'validation_epochs': 25, 'rank': 4, 'with_prior_preservation': False, 'prior_loss_weight': 1.0, 'num_class_images': 100, 'output_dir': 'trained-sd3-lora', 'seed': 0, 'resolution': 512, 'center_crop': False, 'random_flip': False, 'train_text_encoder': False, 'train_batch_size': 1, 'sample_batch_size': 4, 'num_train_epochs': 1, 'max_train_steps': 500, 'checkpointing_steps': 500, 'checkpoints_total_limit': None, 'resume_from_checkpoint': None, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': False, 'learning_rate': 0.0004, 'text_encoder_lr': 5e-06, 'scale_lr': False, 'lr_scheduler': 'constant', 'lr_warmup_steps': 0, 'lr_num_cycles': 1, 'lr_power': 1.0, 'dataloader_num_workers': 0, 'weighting_scheme': 'logit_normal', 'logit_mean': 0.0, 'logit_std': 1.0, 'mode_scale': 1.29, 'precondition_outputs': 1, 'optimizer': 'AdamW', 'use_8bit_adam': False, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'prodigy_beta3': None, 'prodigy_decouple': True, 'adam_weight_decay': 0.0001, 'adam_weight_decay_text_encoder': 0.001, 'lora_layers': None, 'lora_blocks': None, 'adam_epsilon': 1e-08, 'prodigy_use_bias_correction': True, 'prodigy_safeguard_warmup': True, 'max_grad_norm': 1.0, 'push_to_hub': True, 'hub_token': None, 'hub_model_id': None, 'logging_dir': 'logs', 'allow_tf32': False, 'cache_latents': False, 'report_to': 'tensorboard', 'mixed_precision': 'fp16', 'upcast_before_saving': False, 'prior_generation_precision': None, 'local_rank': -1}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Simulate command-line arguments\n",
    "sys.argv = [\n",
    "    \"notebook_name\",\n",
    "    \"--train_text_encoder\",\n",
    "    \"--pretrained_model_name_or_path\", \"stabilityai/stable-diffusion-3.5-medium\",\n",
    "    \"--instance_data_dir\", \"dog\",\n",
    "    \"--output_dir\", \"trained-sd3-lora\",\n",
    "    \"--mixed_precision\", \"fp16\",\n",
    "    \"--instance_prompt\", \"a photo of sks dog\",\n",
    "    \"--resolution\", \"512\",\n",
    "    \"--train_batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--learning_rate\", \"4e-4\",\n",
    "    \"--report_to\", \"tensorboard\",\n",
    "    \"--lr_scheduler\", \"constant\",\n",
    "    \"--lr_warmup_steps\", \"0\",\n",
    "    \"--max_train_steps\", \"500\",\n",
    "    \"--validation_prompt\", \"A photo of sks dog in a bucket\",\n",
    "    \"--validation_epochs\", \"25\",\n",
    "    \"--seed\", \"0\",\n",
    "    \"--push_to_hub\",\n",
    "]\n",
    "\n",
    "from args import parse_args\n",
    "\n",
    "# Parse the arguments\n",
    "args = parse_args()\n",
    "\n",
    "# Use the arguments\n",
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_min_version(\"0.32.0.dev0\")\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        instance_prompt,\n",
    "        class_prompt,\n",
    "        class_data_root=None,\n",
    "        class_num=None,\n",
    "        size=1024,\n",
    "        repeats=1,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.custom_instance_prompts = None\n",
    "        self.class_prompt = class_prompt\n",
    "\n",
    "        # if --dataset_name is provided or a metadata jsonl file is provided in the local --instance_data directory,\n",
    "        # we load the training data using load_dataset\n",
    "        if args.dataset_name is not None:\n",
    "            try:\n",
    "                from datasets import load_dataset\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"You are trying to load your data using the datasets library. If you wish to train using custom \"\n",
    "                    \"captions please install the datasets library: `pip install datasets`. If you wish to load a \"\n",
    "                    \"local folder containing images only, specify --instance_data_dir instead.\"\n",
    "                )\n",
    "            # Downloading and loading a dataset from the hub.\n",
    "            # See more about loading custom images at\n",
    "            # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script\n",
    "            dataset = load_dataset(\n",
    "                args.dataset_name,\n",
    "                args.dataset_config_name,\n",
    "                cache_dir=args.cache_dir,\n",
    "            )\n",
    "            # Preprocessing the datasets.\n",
    "            column_names = dataset[\"train\"].column_names\n",
    "\n",
    "            # 6. Get the column names for input/target.\n",
    "            if args.image_column is None:\n",
    "                image_column = column_names[0]\n",
    "                logger.info(f\"image column defaulting to {image_column}\")\n",
    "            else:\n",
    "                image_column = args.image_column\n",
    "                if image_column not in column_names:\n",
    "                    raise ValueError(\n",
    "                        f\"`--image_column` value '{args.image_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "                    )\n",
    "            instance_images = dataset[\"train\"][image_column]\n",
    "\n",
    "            if args.caption_column is None:\n",
    "                logger.info(\n",
    "                    \"No caption column provided, defaulting to instance_prompt for all images. If your dataset \"\n",
    "                    \"contains captions/prompts for the images, make sure to specify the \"\n",
    "                    \"column as --caption_column\"\n",
    "                )\n",
    "                self.custom_instance_prompts = None\n",
    "            else:\n",
    "                if args.caption_column not in column_names:\n",
    "                    raise ValueError(\n",
    "                        f\"`--caption_column` value '{args.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "                    )\n",
    "                custom_instance_prompts = dataset[\"train\"][args.caption_column]\n",
    "                # create final list of captions according to --repeats\n",
    "                self.custom_instance_prompts = []\n",
    "                for caption in custom_instance_prompts:\n",
    "                    self.custom_instance_prompts.extend(itertools.repeat(caption, repeats))\n",
    "        else:\n",
    "            self.instance_data_root = Path(instance_data_root)\n",
    "            if not self.instance_data_root.exists():\n",
    "                raise ValueError(\"Instance images root doesn't exists.\")\n",
    "\n",
    "            instance_images = [Image.open(path) for path in list(Path(instance_data_root).iterdir())]\n",
    "            self.custom_instance_prompts = None\n",
    "\n",
    "        self.instance_images = []\n",
    "        for img in instance_images:\n",
    "            self.instance_images.extend(itertools.repeat(img, repeats))\n",
    "\n",
    "        self.pixel_values = []\n",
    "        train_resize = transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        train_crop = transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size)\n",
    "        train_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "        train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "        for image in self.instance_images:\n",
    "            image = exif_transpose(image)\n",
    "            if not image.mode == \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            image = train_resize(image)\n",
    "            if args.random_flip and random.random() < 0.5:\n",
    "                # flip\n",
    "                image = train_flip(image)\n",
    "            if args.center_crop:\n",
    "                y1 = max(0, int(round((image.height - args.resolution) / 2.0)))\n",
    "                x1 = max(0, int(round((image.width - args.resolution) / 2.0)))\n",
    "                image = train_crop(image)\n",
    "            else:\n",
    "                y1, x1, h, w = train_crop.get_params(image, (args.resolution, args.resolution))\n",
    "                image = crop(image, y1, x1, h, w)\n",
    "            image = train_transforms(image)\n",
    "            self.pixel_values.append(image)\n",
    "\n",
    "        self.num_instance_images = len(self.instance_images)\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(self.class_data_root.iterdir())\n",
    "            if class_num is not None:\n",
    "                self.num_class_images = min(len(self.class_images_path), class_num)\n",
    "            else:\n",
    "                self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = self.pixel_values[index % self.num_instance_images]\n",
    "        example[\"instance_images\"] = instance_image\n",
    "\n",
    "        if self.custom_instance_prompts:\n",
    "            caption = self.custom_instance_prompts[index % self.num_instance_images]\n",
    "            if caption:\n",
    "                example[\"instance_prompt\"] = caption\n",
    "            else:\n",
    "                example[\"instance_prompt\"] = self.instance_prompt\n",
    "\n",
    "        else:  # custom prompts were provided, but length does not match size of image dataset\n",
    "            example[\"instance_prompt\"] = self.instance_prompt\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            class_image = exif_transpose(class_image)\n",
    "\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt\"] = self.class_prompt\n",
    "\n",
    "        return example\n",
    "\n",
    "def collate_fn(examples, with_prior_preservation=False):\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    prompts = [example[\"instance_prompt\"] for example in examples]\n",
    "\n",
    "    # Concat class and instance examples for prior preservation.\n",
    "    # We do this to avoid doing two forward passes.\n",
    "    if with_prior_preservation:\n",
    "        pixel_values += [example[\"class_images\"] for example in examples]\n",
    "        prompts += [example[\"class_prompt\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    batch = {\"pixel_values\": pixel_values, \"prompts\": prompts}\n",
    "    return batch\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n",
    "\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "11/19/2024 16:24:54 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.report_to == \"wandb\" and args.hub_token is not None:\n",
    "    raise ValueError(\n",
    "        \"You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token.\"\n",
    "        \" Please use `huggingface-cli login` to authenticate with the Hub.\"\n",
    "    )\n",
    "\n",
    "if torch.backends.mps.is_available() and args.mixed_precision == \"bf16\":\n",
    "    # due to pytorch#99272, MPS does not yet support bfloat16.\n",
    "    raise ValueError(\n",
    "        \"Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead.\"\n",
    "    )\n",
    "\n",
    "logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    "    kwargs_handlers=[kwargs],\n",
    ")\n",
    "\n",
    "# Disable AMP for MPS.\n",
    "if torch.backends.mps.is_available():\n",
    "    accelerator.native_amp = False\n",
    "\n",
    "if args.report_to == \"wandb\":\n",
    "    if not is_wandb_available():\n",
    "        raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate class images if prior preservation is enabled.\n",
    "if args.with_prior_preservation:\n",
    "    class_images_dir = Path(args.class_data_dir)\n",
    "    if not class_images_dir.exists():\n",
    "        class_images_dir.mkdir(parents=True)\n",
    "    cur_class_images = len(list(class_images_dir.iterdir()))\n",
    "\n",
    "    if cur_class_images < args.num_class_images:\n",
    "        has_supported_fp16_accelerator = torch.cuda.is_available() or torch.backends.mps.is_available()\n",
    "        torch_dtype = torch.float16 if has_supported_fp16_accelerator else torch.float32\n",
    "        if args.prior_generation_precision == \"fp32\":\n",
    "            torch_dtype = torch.float32\n",
    "        elif args.prior_generation_precision == \"fp16\":\n",
    "            torch_dtype = torch.float16\n",
    "        elif args.prior_generation_precision == \"bf16\":\n",
    "            torch_dtype = torch.bfloat16\n",
    "        pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            revision=args.revision,\n",
    "            variant=args.variant,\n",
    "        )\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "        num_new_images = args.num_class_images - cur_class_images\n",
    "        logger.info(f\"Number of class images to sample: {num_new_images}.\")\n",
    "\n",
    "        sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n",
    "        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n",
    "\n",
    "        sample_dataloader = accelerator.prepare(sample_dataloader)\n",
    "        pipeline.to(accelerator.device)\n",
    "\n",
    "        for example in tqdm(\n",
    "            sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n",
    "        ):\n",
    "            images = pipeline(example[\"prompt\"]).images\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "                hash_image = insecure_hashlib.sha1(image.tobytes()).hexdigest()\n",
    "                image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n",
    "                image.save(image_filename)\n",
    "\n",
    "        del pipeline\n",
    "        free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        repo_id = create_repo(\n",
    "            repo_id=args.hub_model_id or Path(args.output_dir).name,\n",
    "            exist_ok=True,\n",
    "        ).repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers\n",
    "from models import load_tokenizers\n",
    "\n",
    "tokenizer_one, tokenizer_two, tokenizer_three = load_tokenizers(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# Import correct text encoder\n",
    "from models import import_model_class_from_model_name_or_path\n",
    "\n",
    "text_encoder_cls_one = import_model_class_from_model_name_or_path(\n",
    "    args.pretrained_model_name_or_path, args.revision\n",
    ")\n",
    "text_encoder_cls_two = import_model_class_from_model_name_or_path(\n",
    "    args.pretrained_model_name_or_path, args.revision, subfolder=\"text_encoder_2\"\n",
    ")\n",
    "text_encoder_cls_three = import_model_class_from_model_name_or_path(\n",
    "    args.pretrained_model_name_or_path, args.revision, subfolder=\"text_encoder_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'base_shift', 'invert_sigmas', 'base_image_seq_len', 'use_dynamic_shifting', 'max_shift', 'max_image_seq_len'} was not found in config. Values will be initialized to default values.\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 3315.66it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load scheduler and models\n",
    "from models import load_text_encoders\n",
    "\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")\n",
    "noise_scheduler_copy = copy.deepcopy(noise_scheduler)\n",
    "text_encoder_one, text_encoder_two, text_encoder_three = load_text_encoders(\n",
    "    text_encoder_cls_one, text_encoder_cls_two, text_encoder_cls_three, args\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"vae\",\n",
    "    revision=args.revision,\n",
    "    variant=args.variant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SD3Transformer2DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"transformer\", revision=args.revision, variant=args.variant\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(32128, 4096)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 4096)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 64)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "text_encoder_three.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora transformer) to half-precision\n",
    "# as these weights are only used for inference, keeping weights in full precision is not required.\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "if torch.backends.mps.is_available() and weight_dtype == torch.bfloat16:\n",
    "    # due to pytorch#99272, MPS does not yet support bfloat16.\n",
    "    raise ValueError(\n",
    "        \"Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator.device cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(32128, 4096)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 4096)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 64)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"accelerator.device\", accelerator.device)\n",
    "vae.to(accelerator.device, dtype=torch.float32)\n",
    "transformer.to(accelerator.device, dtype=weight_dtype)\n",
    "text_encoder_one.to(accelerator.device, dtype=weight_dtype)\n",
    "text_encoder_two.to(accelerator.device, dtype=weight_dtype)\n",
    "text_encoder_three.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gradient_checkpointing:\n",
    "    transformer.enable_gradient_checkpointing()\n",
    "    if args.train_text_encoder:\n",
    "        text_encoder_one.gradient_checkpointing_enable()\n",
    "        text_encoder_two.gradient_checkpointing_enable()\n",
    "if args.lora_layers is not None:\n",
    "    target_modules = [layer.strip() for layer in args.lora_layers.split(\",\")]\n",
    "else:\n",
    "    target_modules = [\n",
    "        \"attn.add_k_proj\",\n",
    "        \"attn.add_q_proj\",\n",
    "        \"attn.add_v_proj\",\n",
    "        \"attn.to_add_out\",\n",
    "        \"attn.to_k\",\n",
    "        \"attn.to_out.0\",\n",
    "        \"attn.to_q\",\n",
    "        \"attn.to_v\",\n",
    "    ]\n",
    "\n",
    "if args.lora_blocks is not None:\n",
    "    target_blocks = [int(block.strip()) for block in args.lora_blocks.split(\",\")]\n",
    "    target_modules = [\n",
    "        f\"transformer_blocks.{block}.{module}\" for block in target_blocks for module in target_modules\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will add new LoRA weights to the attention layers\n",
    "transformer_lora_config = LoraConfig(\n",
    "    r=args.rank,\n",
    "    lora_alpha=args.rank,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "transformer.add_adapter(transformer_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.train_text_encoder:\n",
    "    text_lora_config = LoraConfig(\n",
    "        r=args.rank,\n",
    "        lora_alpha=args.rank,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "    )\n",
    "    text_encoder_one.add_adapter(text_lora_config)\n",
    "    text_encoder_two.add_adapter(text_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f956fbc82e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unwrap_model(model):\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model = model._orig_mod if is_compiled_module(model) else model\n",
    "    return model\n",
    "\n",
    "# create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n",
    "def save_model_hook(models, weights, output_dir):\n",
    "    if accelerator.is_main_process:\n",
    "        transformer_lora_layers_to_save = None\n",
    "        text_encoder_one_lora_layers_to_save = None\n",
    "        text_encoder_two_lora_layers_to_save = None\n",
    "\n",
    "        for model in models:\n",
    "            if isinstance(model, type(unwrap_model(transformer))):\n",
    "                transformer_lora_layers_to_save = get_peft_model_state_dict(model)\n",
    "            elif isinstance(model, type(unwrap_model(text_encoder_one))):\n",
    "                text_encoder_one_lora_layers_to_save = get_peft_model_state_dict(model)\n",
    "            elif isinstance(model, type(unwrap_model(text_encoder_two))):\n",
    "                text_encoder_two_lora_layers_to_save = get_peft_model_state_dict(model)\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "            # make sure to pop weight so that corresponding model is not saved again\n",
    "            weights.pop()\n",
    "\n",
    "        StableDiffusion3Pipeline.save_lora_weights(\n",
    "            output_dir,\n",
    "            transformer_lora_layers=transformer_lora_layers_to_save,\n",
    "            text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "            text_encoder_2_lora_layers=text_encoder_two_lora_layers_to_save,\n",
    "        )\n",
    "\n",
    "def load_model_hook(models, input_dir):\n",
    "    transformer_ = None\n",
    "    text_encoder_one_ = None\n",
    "    text_encoder_two_ = None\n",
    "\n",
    "    while len(models) > 0:\n",
    "        model = models.pop()\n",
    "\n",
    "        if isinstance(model, type(unwrap_model(transformer))):\n",
    "            transformer_ = model\n",
    "        elif isinstance(model, type(unwrap_model(text_encoder_one))):\n",
    "            text_encoder_one_ = model\n",
    "        elif isinstance(model, type(unwrap_model(text_encoder_two))):\n",
    "            text_encoder_two_ = model\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "    lora_state_dict = StableDiffusion3Pipeline.lora_state_dict(input_dir)\n",
    "\n",
    "    transformer_state_dict = {\n",
    "        f'{k.replace(\"transformer.\", \"\")}': v for k, v in lora_state_dict.items() if k.startswith(\"transformer.\")\n",
    "    }\n",
    "    transformer_state_dict = convert_unet_state_dict_to_peft(transformer_state_dict)\n",
    "    incompatible_keys = set_peft_model_state_dict(transformer_, transformer_state_dict, adapter_name=\"default\")\n",
    "    if incompatible_keys is not None:\n",
    "        # check only for unexpected keys\n",
    "        unexpected_keys = getattr(incompatible_keys, \"unexpected_keys\", None)\n",
    "        if unexpected_keys:\n",
    "            logger.warning(\n",
    "                f\"Loading adapter weights from state_dict led to unexpected keys not found in the model: \"\n",
    "                f\" {unexpected_keys}. \"\n",
    "            )\n",
    "    if args.train_text_encoder:\n",
    "        # Do we need to call `scale_lora_layers()` here?\n",
    "        _set_state_dict_into_text_encoder(lora_state_dict, prefix=\"text_encoder.\", text_encoder=text_encoder_one_)\n",
    "\n",
    "        _set_state_dict_into_text_encoder(\n",
    "            lora_state_dict, prefix=\"text_encoder_2.\", text_encoder=text_encoder_two_\n",
    "        )\n",
    "\n",
    "    # Make sure the trainable params are in float32. This is again needed since the base models\n",
    "    # are in `weight_dtype`. More details:\n",
    "    # https://github.com/huggingface/diffusers/pull/6514#discussion_r1449796804\n",
    "    if args.mixed_precision == \"fp16\":\n",
    "        models = [transformer_]\n",
    "        if args.train_text_encoder:\n",
    "            models.extend([text_encoder_one_, text_encoder_two_])\n",
    "        # only upcast trainable parameters (LoRA) into fp32\n",
    "        cast_training_params(models)\n",
    "\n",
    "accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "accelerator.register_load_state_pre_hook(load_model_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable TF32 for faster training on Ampere GPUs,\n",
    "# cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "if args.allow_tf32 and torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "if args.scale_lr:\n",
    "    args.learning_rate = (\n",
    "        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the trainable params are in float32.\n",
    "if args.mixed_precision == \"fp16\":\n",
    "    models = [transformer]\n",
    "    if args.train_text_encoder:\n",
    "        models.extend([text_encoder_one, text_encoder_two])\n",
    "    # only upcast trainable parameters (LoRA) into fp32\n",
    "    cast_training_params(models, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))\n",
    "if args.train_text_encoder:\n",
    "    text_lora_parameters_one = list(filter(lambda p: p.requires_grad, text_encoder_one.parameters()))\n",
    "    text_lora_parameters_two = list(filter(lambda p: p.requires_grad, text_encoder_two.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "transformer_parameters_with_lr = {\"params\": transformer_lora_parameters, \"lr\": args.learning_rate}\n",
    "if args.train_text_encoder:\n",
    "    # different learning rate for text encoder and unet\n",
    "    text_lora_parameters_one_with_lr = {\n",
    "        \"params\": text_lora_parameters_one,\n",
    "        \"weight_decay\": args.adam_weight_decay_text_encoder,\n",
    "        \"lr\": args.text_encoder_lr if args.text_encoder_lr else args.learning_rate,\n",
    "    }\n",
    "    text_lora_parameters_two_with_lr = {\n",
    "        \"params\": text_lora_parameters_two,\n",
    "        \"weight_decay\": args.adam_weight_decay_text_encoder,\n",
    "        \"lr\": args.text_encoder_lr if args.text_encoder_lr else args.learning_rate,\n",
    "    }\n",
    "    params_to_optimize = [\n",
    "        transformer_parameters_with_lr,\n",
    "        text_lora_parameters_one_with_lr,\n",
    "        text_lora_parameters_two_with_lr,\n",
    "    ]\n",
    "else:\n",
    "    params_to_optimize = [transformer_parameters_with_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer creation\n",
    "if not (args.optimizer.lower() == \"prodigy\" or args.optimizer.lower() == \"adamw\"):\n",
    "    logger.warning(\n",
    "        f\"Unsupported choice of optimizer: {args.optimizer}.Supported optimizers include [adamW, prodigy].\"\n",
    "        \"Defaulting to adamW\"\n",
    "    )\n",
    "    args.optimizer = \"adamw\"\n",
    "\n",
    "if args.use_8bit_adam and not args.optimizer.lower() == \"adamw\":\n",
    "    logger.warning(\n",
    "        f\"use_8bit_adam is ignored when optimizer is not set to 'AdamW'. Optimizer was \"\n",
    "        f\"set to {args.optimizer.lower()}\"\n",
    "    )\n",
    "\n",
    "if args.optimizer.lower() == \"adamw\":\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "            )\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "if args.optimizer.lower() == \"prodigy\":\n",
    "    try:\n",
    "        import prodigyopt\n",
    "    except ImportError:\n",
    "        raise ImportError(\"To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`\")\n",
    "\n",
    "    optimizer_class = prodigyopt.Prodigy\n",
    "\n",
    "    if args.learning_rate <= 0.1:\n",
    "        logger.warning(\n",
    "            \"Learning rate is too low. When using prodigy, it's generally better to set learning rate around 1.0\"\n",
    "        )\n",
    "    if args.train_text_encoder and args.text_encoder_lr:\n",
    "        logger.warning(\n",
    "            f\"Learning rates were provided both for the transformer and the text encoder- e.g. text_encoder_lr:\"\n",
    "            f\" {args.text_encoder_lr} and learning_rate: {args.learning_rate}. \"\n",
    "            f\"When using prodigy only learning_rate is used as the initial learning rate.\"\n",
    "        )\n",
    "        # changes the learning rate of text_encoder_parameters_one and text_encoder_parameters_two to be\n",
    "        # --learning_rate\n",
    "        params_to_optimize[1][\"lr\"] = args.learning_rate\n",
    "        params_to_optimize[2][\"lr\"] = args.learning_rate\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        beta3=args.prodigy_beta3,\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "        decouple=args.prodigy_decouple,\n",
    "        use_bias_correction=args.prodigy_use_bias_correction,\n",
    "        safeguard_warmup=args.prodigy_safeguard_warmup,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoaders creation:\n",
    "train_dataset = DreamBoothDataset(\n",
    "    instance_data_root=args.instance_data_dir,\n",
    "    instance_prompt=args.instance_prompt,\n",
    "    class_prompt=args.class_prompt,\n",
    "    class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
    "    class_num=args.num_class_images,\n",
    "    size=args.resolution,\n",
    "    repeats=args.repeats,\n",
    "    center_crop=args.center_crop,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import encode_prompt\n",
    "\n",
    "if not args.train_text_encoder:\n",
    "    tokenizers = [tokenizer_one, tokenizer_two, tokenizer_three]\n",
    "    text_encoders = [text_encoder_one, text_encoder_two, text_encoder_three]\n",
    "\n",
    "    def compute_text_embeddings(prompt, text_encoders, tokenizers):\n",
    "        with torch.no_grad():\n",
    "            prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                text_encoders, tokenizers, prompt, args.max_sequence_length\n",
    "            )\n",
    "            prompt_embeds = prompt_embeds.to(accelerator.device)\n",
    "            pooled_prompt_embeds = pooled_prompt_embeds.to(accelerator.device)\n",
    "        return prompt_embeds, pooled_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no type of tuning is done on the text_encoder and custom instance prompts are NOT\n",
    "# provided (i.e. the --instance_prompt is used for all images), we encode the instance prompt once to avoid\n",
    "# the redundant encoding.\n",
    "if not args.train_text_encoder and not train_dataset.custom_instance_prompts:\n",
    "    instance_prompt_hidden_states, instance_pooled_prompt_embeds = compute_text_embeddings(\n",
    "        args.instance_prompt, text_encoders, tokenizers\n",
    "    )\n",
    "\n",
    "# Handle class prompt for prior-preservation.\n",
    "if args.with_prior_preservation:\n",
    "    if not args.train_text_encoder:\n",
    "        class_prompt_hidden_states, class_pooled_prompt_embeds = compute_text_embeddings(\n",
    "            args.class_prompt, text_encoders, tokenizers\n",
    "        )\n",
    "\n",
    "# Clear the memory here\n",
    "if not args.train_text_encoder and not train_dataset.custom_instance_prompts:\n",
    "    # Explicitly delete the objects as well, otherwise only the lists are deleted and the original references remain, preventing garbage collection\n",
    "    del tokenizers, text_encoders\n",
    "    del text_encoder_one, text_encoder_two, text_encoder_three\n",
    "    free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import tokenize_prompt\n",
    "# If custom instance prompts are NOT provided (i.e. the instance prompt is used for all images),\n",
    "# pack the statically computed variables appropriately here. This is so that we don't\n",
    "# have to pass them to the dataloader.\n",
    "\n",
    "if not train_dataset.custom_instance_prompts:\n",
    "    if not args.train_text_encoder:\n",
    "        prompt_embeds = instance_prompt_hidden_states\n",
    "        pooled_prompt_embeds = instance_pooled_prompt_embeds\n",
    "        if args.with_prior_preservation:\n",
    "            prompt_embeds = torch.cat([prompt_embeds, class_prompt_hidden_states], dim=0)\n",
    "            pooled_prompt_embeds = torch.cat([pooled_prompt_embeds, class_pooled_prompt_embeds], dim=0)\n",
    "        # if we're optimizing the text encoder (both if instance prompt is used for all images or custom prompts) we need to tokenize and encode the\n",
    "    # batch prompts on all training steps\n",
    "    else:\n",
    "        tokens_one = tokenize_prompt(tokenizer_one, args.instance_prompt)\n",
    "        tokens_two = tokenize_prompt(tokenizer_two, args.instance_prompt)\n",
    "        tokens_three = tokenize_prompt(tokenizer_three, args.instance_prompt)\n",
    "        if args.with_prior_preservation:\n",
    "            class_tokens_one = tokenize_prompt(tokenizer_one, args.class_prompt)\n",
    "            class_tokens_two = tokenize_prompt(tokenizer_two, args.class_prompt)\n",
    "            class_tokens_three = tokenize_prompt(tokenizer_three, args.class_prompt)\n",
    "            tokens_one = torch.cat([tokens_one, class_tokens_one], dim=0)\n",
    "            tokens_two = torch.cat([tokens_two, class_tokens_two], dim=0)\n",
    "            tokens_three = torch.cat([tokens_three, class_tokens_three], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_config_shift_factor = vae.config.shift_factor\n",
    "vae_config_scaling_factor = vae.config.scaling_factor\n",
    "if args.cache_latents:\n",
    "    latents_cache = []\n",
    "    for batch in tqdm(train_dataloader, desc=\"Caching latents\"):\n",
    "        with torch.no_grad():\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(\n",
    "                accelerator.device, non_blocking=True, dtype=weight_dtype\n",
    "            )\n",
    "            latents_cache.append(vae.encode(batch[\"pixel_values\"]).latent_dist)\n",
    "\n",
    "    if args.validation_prompt is None:\n",
    "        del vae\n",
    "        free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    "    num_cycles=args.lr_num_cycles,\n",
    "    power=args.lr_power,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "if args.train_text_encoder:\n",
    "    (\n",
    "        transformer,\n",
    "        text_encoder_one,\n",
    "        text_encoder_two,\n",
    "        optimizer,\n",
    "        train_dataloader,\n",
    "        lr_scheduler,\n",
    "    ) = accelerator.prepare(\n",
    "        transformer, text_encoder_one, text_encoder_two, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    assert text_encoder_one is not None\n",
    "    assert text_encoder_two is not None\n",
    "    assert text_encoder_three is not None\n",
    "else:\n",
    "    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        transformer, optimizer, train_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if accelerator.is_main_process:\n",
    "    tracker_name = \"dreambooth-sd3-lora\"\n",
    "    accelerator.init_trackers(tracker_name, config=vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/19/2024 16:25:21 - INFO - __main__ - ***** Running training *****\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Num examples = 5\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Num batches each epoch = 5\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Num Epochs = 250\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "11/19/2024 16:25:21 - INFO - __main__ -   Total optimization steps = 500\n"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the mos recent checkpoint\n",
    "        dirs = os.listdir(args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        args.resume_from_checkpoint = None\n",
    "        initial_global_step = 0\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "        initial_global_step = global_step\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "\n",
    "else:\n",
    "    initial_global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(\n",
    "    range(0, args.max_train_steps),\n",
    "    initial=initial_global_step,\n",
    "    desc=\"Steps\",\n",
    "    # Only show the progress bar once on each machine.\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):\n",
    "    sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)\n",
    "    schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)\n",
    "    timesteps = timesteps.to(accelerator.device)\n",
    "    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n",
    "\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    while len(sigma.shape) < n_dim:\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(\n",
    "    pipeline,\n",
    "    args,\n",
    "    accelerator,\n",
    "    pipeline_args,\n",
    "    epoch,\n",
    "    torch_dtype,\n",
    "    is_final_validation=False,\n",
    "    logger=None,\n",
    "):\n",
    "    logger.info(\n",
    "        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "        f\" {args.validation_prompt}.\"\n",
    "    )\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # run inference\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None\n",
    "    # autocast_ctx = torch.autocast(accelerator.device.type) if not is_final_validation else nullcontext()\n",
    "    autocast_ctx = nullcontext()\n",
    "\n",
    "    with autocast_ctx:\n",
    "        images = [pipeline(**pipeline_args, generator=generator).images[0] for _ in range(args.num_validation_images)]\n",
    "\n",
    "    for tracker in accelerator.trackers:\n",
    "        phase_name = \"test\" if is_final_validation else \"validation\"\n",
    "        if tracker.name == \"tensorboard\":\n",
    "            np_images = np.stack([np.asarray(img) for img in images])\n",
    "            tracker.writer.add_images(phase_name, np_images, epoch, dataformats=\"NHWC\")\n",
    "        if tracker.name == \"wandb\":\n",
    "            tracker.log(\n",
    "                {\n",
    "                    phase_name: [\n",
    "                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images)\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    del pipeline\n",
    "    free_memory()\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "\n",
    "for epoch in range(first_epoch, args.num_train_epochs):\n",
    "    transformer.train()\n",
    "    if args.train_text_encoder:\n",
    "        print(\"Training text encoders\")\n",
    "        text_encoder_one.train()\n",
    "        text_encoder_two.train()\n",
    "        \n",
    "        # set top parameter requires_grad = True for gradient checkpointing works\n",
    "        accelerator.unwrap_model(text_encoder_one).text_model.embeddings.requires_grad_(True)\n",
    "        accelerator.unwrap_model(text_encoder_two).text_model.embeddings.requires_grad_(True)\n",
    "\n",
    "    # accumulated_gradients = {name: torch.zeros_like(param) for name, param in transformer.named_parameters()}\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        models_to_accumulate = [transformer]\n",
    "        if args.train_text_encoder:\n",
    "            models_to_accumulate.extend([text_encoder_one, text_encoder_two])\n",
    "        with accelerator.accumulate(models_to_accumulate):\n",
    "            prompts = batch[\"prompts\"]\n",
    "\n",
    "            # encode batch prompts when custom prompts are provided for each image -\n",
    "            if train_dataset.custom_instance_prompts:\n",
    "                if not args.train_text_encoder:\n",
    "                    prompt_embeds, pooled_prompt_embeds = compute_text_embeddings(\n",
    "                        prompts, text_encoders, tokenizers\n",
    "                    )\n",
    "                else:\n",
    "                    tokens_one = tokenize_prompt(tokenizer_one, prompts)\n",
    "                    tokens_two = tokenize_prompt(tokenizer_two, prompts)\n",
    "                    tokens_three = tokenize_prompt(tokenizer_three, prompts)\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one, text_encoder_two, text_encoder_three],\n",
    "                        tokenizers=[None, None, None],\n",
    "                        prompt=prompts,\n",
    "                        max_sequence_length=args.max_sequence_length,\n",
    "                        text_input_ids_list=[tokens_one, tokens_two, tokens_three],\n",
    "                    )\n",
    "            else:\n",
    "                if args.train_text_encoder:\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one, text_encoder_two, text_encoder_three],\n",
    "                        tokenizers=[None, None, tokenizer_three],\n",
    "                        prompt=args.instance_prompt,\n",
    "                        max_sequence_length=args.max_sequence_length,\n",
    "                        text_input_ids_list=[tokens_one, tokens_two, tokens_three],\n",
    "                    )\n",
    "\n",
    "            # Convert images to latent space\n",
    "            if args.cache_latents:\n",
    "                model_input = latents_cache[step].sample()\n",
    "            else:\n",
    "                pixel_values = batch[\"pixel_values\"].to(dtype=vae.dtype)\n",
    "                model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "\n",
    "            model_input = (model_input - vae_config_shift_factor) * vae_config_scaling_factor\n",
    "            model_input = model_input.to(dtype=weight_dtype)\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(model_input)\n",
    "            bsz = model_input.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            # for weighting schemes where we sample timesteps non-uniformly\n",
    "            u = compute_density_for_timestep_sampling(\n",
    "                weighting_scheme=args.weighting_scheme,\n",
    "                batch_size=bsz,\n",
    "                logit_mean=args.logit_mean,\n",
    "                logit_std=args.logit_std,\n",
    "                mode_scale=args.mode_scale,\n",
    "            )\n",
    "            indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()\n",
    "            timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)\n",
    "\n",
    "            # Add noise according to flow matching.\n",
    "            # zt = (1 - texp) * x + texp * z1\n",
    "            sigmas = get_sigmas(timesteps, n_dim=model_input.ndim, dtype=model_input.dtype)\n",
    "            noisy_model_input = (1.0 - sigmas) * model_input + sigmas * noise\n",
    "\n",
    "            # Predict the noise residual\n",
    "            model_pred = transformer(\n",
    "                hidden_states=noisy_model_input,\n",
    "                timestep=timesteps,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                pooled_projections=pooled_prompt_embeds,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # Follow: Section 5 of https://arxiv.org/abs/2206.00364.\n",
    "            # Preconditioning of the model outputs.\n",
    "            if args.precondition_outputs:\n",
    "                model_pred = model_pred * (-sigmas) + noisy_model_input\n",
    "\n",
    "            # these weighting schemes use a uniform timestep sampling\n",
    "            # and instead post-weight the loss\n",
    "            weighting = compute_loss_weighting_for_sd3(weighting_scheme=args.weighting_scheme, sigmas=sigmas)\n",
    "\n",
    "            # flow matching loss\n",
    "            if args.precondition_outputs:\n",
    "                target = model_input\n",
    "            else:\n",
    "                target = noise - model_input\n",
    "\n",
    "            if args.with_prior_preservation:\n",
    "                # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n",
    "                model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "                target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "                # Compute prior loss\n",
    "                prior_loss = torch.mean(\n",
    "                    (weighting.float() * (model_pred_prior.float() - target_prior.float()) ** 2).reshape(\n",
    "                        target_prior.shape[0], -1\n",
    "                    ),\n",
    "                    1,\n",
    "                )\n",
    "                prior_loss = prior_loss.mean()\n",
    "\n",
    "            # Compute regular loss.\n",
    "            loss = torch.mean(\n",
    "                (weighting.float() * (model_pred.float() - target.float()) ** 2).reshape(target.shape[0], -1),\n",
    "                1,\n",
    "            )\n",
    "            loss = loss.mean()\n",
    "\n",
    "            if args.with_prior_preservation:\n",
    "                # Add the prior loss to the instance loss.\n",
    "                loss = loss + args.prior_loss_weight * prior_loss\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.is_main_process:\n",
    "                gradients = {}\n",
    "                def save_model_gradients(model, model_name):\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            gradients[f\"{model_name}.{name}\"] = param.grad.clone().detach().cpu()\n",
    "                save_model_gradients(transformer, \"transformer\")\n",
    "                if args.train_text_encoder:\n",
    "                    save_model_gradients(text_encoder_one, \"text_encoder_one\")\n",
    "                    save_model_gradients(text_encoder_two, \"text_encoder_two\")\n",
    "                torch.save(gradients, f\"gradients_{step}.pt\")\n",
    "                break\n",
    "            \n",
    "            if accelerator.sync_gradients:\n",
    "                params_to_clip = (\n",
    "                    itertools.chain(\n",
    "                        transformer_lora_parameters, text_lora_parameters_one, text_lora_parameters_two\n",
    "                    )\n",
    "                    if args.train_text_encoder\n",
    "                    else transformer_lora_parameters\n",
    "                )\n",
    "                accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                    if args.checkpoints_total_limit is not None:\n",
    "                        checkpoints = os.listdir(args.output_dir)\n",
    "                        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                        if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                            num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                            removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                            logger.info(\n",
    "                                f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                            )\n",
    "                            logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                            for removing_checkpoint in removing_checkpoints:\n",
    "                                removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n",
    "                                shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "    \n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    # if accelerator.is_main_process:\n",
    "    #     if args.validation_prompt is not None and epoch % args.validation_epochs == 0:\n",
    "    #         if not args.train_text_encoder:\n",
    "    #             # create pipeline\n",
    "    #             text_encoder_one, text_encoder_two, text_encoder_three = load_text_encoders(\n",
    "    #                 text_encoder_cls_one, text_encoder_cls_two, text_encoder_cls_three, args\n",
    "    #             )\n",
    "    #             text_encoder_one.to(weight_dtype)\n",
    "    #             text_encoder_two.to(weight_dtype)\n",
    "    #         pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "    #             args.pretrained_model_name_or_path,\n",
    "    #             vae=vae,\n",
    "    #             text_encoder=accelerator.unwrap_model(text_encoder_one),\n",
    "    #             text_encoder_2=accelerator.unwrap_model(text_encoder_two),\n",
    "    #             text_encoder_3=accelerator.unwrap_model(text_encoder_three),\n",
    "    #             transformer=accelerator.unwrap_model(transformer),\n",
    "    #             revision=args.revision,\n",
    "    #             variant=args.variant,\n",
    "    #             torch_dtype=weight_dtype,\n",
    "    #         )\n",
    "    #         pipeline_args = {\"prompt\": args.validation_prompt}\n",
    "\n",
    "    #         images = log_validation(\n",
    "    #             pipeline=pipeline,\n",
    "    #             args=args,\n",
    "    #             accelerator=accelerator,\n",
    "    #             pipeline_args=pipeline_args,\n",
    "    #             epoch=epoch,\n",
    "    #             torch_dtype=weight_dtype,\n",
    "    #             logger=logger,\n",
    "    #         )\n",
    "    #         if not args.train_text_encoder:\n",
    "    #             del text_encoder_one, text_encoder_two, text_encoder_three\n",
    "    #             free_memory()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hf import save_model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in trained-sd3-lora/pytorch_lora_weights.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'base_shift', 'invert_sigmas', 'base_image_seq_len', 'use_dynamic_shifting', 'max_shift', 'max_image_seq_len'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "Loaded text_encoder_3 as T5EncoderModel from `text_encoder_3` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded text_encoder as CLIPTextModelWithProjection from `text_encoder` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded transformer as SD3Transformer2DModel from `transformer` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3.5-medium.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:10<00:00,  1.13s/it]\n",
      "Loading transformer.\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "events.out.tfevents.1732055121.terminator5.cs.rice.edu.3762708.0: 100%|██████████| 2.95k/2.95k [00:00<00:00, 26.3kB/s]\n",
      "events.out.tfevents.1732055121.terminator5.cs.rice.edu.3762708.1: 100%|██████████| 3.33k/3.33k [00:00<00:00, 25.8kB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|██████████| 4.74M/4.74M [00:00<00:00, 10.6MB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload 3 LFS files: 100%|██████████| 3/3 [00:00<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the lora layers\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    transformer = unwrap_model(transformer)\n",
    "    if args.upcast_before_saving:\n",
    "        transformer.to(torch.float32)\n",
    "    else:\n",
    "        transformer = transformer.to(weight_dtype)\n",
    "    transformer_lora_layers = get_peft_model_state_dict(transformer)\n",
    "\n",
    "    if args.train_text_encoder:\n",
    "        text_encoder_one = unwrap_model(text_encoder_one)\n",
    "        text_encoder_lora_layers = get_peft_model_state_dict(text_encoder_one.to(torch.float32))\n",
    "        text_encoder_two = unwrap_model(text_encoder_two)\n",
    "        text_encoder_2_lora_layers = get_peft_model_state_dict(text_encoder_two.to(torch.float32))\n",
    "    else:\n",
    "        text_encoder_lora_layers = None\n",
    "        text_encoder_2_lora_layers = None\n",
    "\n",
    "    StableDiffusion3Pipeline.save_lora_weights(\n",
    "        save_directory=args.output_dir,\n",
    "        transformer_lora_layers=transformer_lora_layers,\n",
    "        text_encoder_lora_layers=text_encoder_lora_layers,\n",
    "        text_encoder_2_lora_layers=text_encoder_2_lora_layers,\n",
    "    )\n",
    "\n",
    "    # Final inference\n",
    "    # Load previous pipeline\n",
    "    pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        revision=args.revision,\n",
    "        variant=args.variant,\n",
    "        torch_dtype=weight_dtype,\n",
    "    )\n",
    "    # load attention processors\n",
    "    pipeline.load_lora_weights(args.output_dir)\n",
    "\n",
    "    # run inference\n",
    "    images = []\n",
    "    if args.validation_prompt and args.num_validation_images > 0:\n",
    "        pipeline_args = {\"prompt\": args.validation_prompt}\n",
    "        # images = log_validation(\n",
    "        #     pipeline=pipeline,\n",
    "        #     args=args,\n",
    "        #     accelerator=accelerator,\n",
    "        #     pipeline_args=pipeline_args,\n",
    "        #     epoch=epoch,\n",
    "        #     is_final_validation=True,\n",
    "        #     torch_dtype=weight_dtype,\n",
    "        # )\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        # save_model_card(\n",
    "        #     repo_id,\n",
    "        #     images=images,\n",
    "        #     base_model=args.pretrained_model_name_or_path,\n",
    "        #     instance_prompt=args.instance_prompt,\n",
    "        #     validation_prompt=args.validation_prompt,\n",
    "        #     train_text_encoder=args.train_text_encoder,\n",
    "        #     repo_folder=args.output_dir,\n",
    "        # )\n",
    "        upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=args.output_dir,\n",
    "            commit_message=\"End of training\",\n",
    "            ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "        )\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
