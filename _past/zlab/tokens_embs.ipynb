{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import CLIPTokenizer, T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# tokenizers\n",
    "pretrained_model_name = \"stabilityai/stable-diffusion-3.5-medium\"\n",
    "revision = None\n",
    "\n",
    "tokenizer_clip_l = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=revision,\n",
    ")\n",
    "tokenizer_clip_g = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"tokenizer_2\",\n",
    "    revision=revision,\n",
    ")\n",
    "tokenizer_t5 = T5TokenizerFast.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    subfolder=\"tokenizer_3\",\n",
    "    revision=revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['I am Bob', 'Bob is a myself']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    return text_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clip_l = tokenizer_clip_l(prompts, return_tensors=\"pt\", padding=True)\n",
    "tokens_clip_g = tokenizer_clip_g(prompts, return_tensors=\"pt\", padding=True)\n",
    "tokens_t5 = tokenizer_t5(prompts, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,   328,   687,  4423, 49407, 49407],\n",
      "        [49406,  4423,   533,   320,  3245, 49407]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_clip_l['input_ids'])\n",
    "print(tokens_clip_l['input_ids'].shape)\n",
    "print(tokens_clip_g['attention_mask'])\n",
    "print(tokens_clip_g['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(49406, 1.0), (328, 1.0), (687, 1.0), (4423, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0), (49407, 1.0)]]\n"
     ]
    }
   ],
   "source": [
    "from sd3.stability.other_impls import SD3Tokenizer\n",
    "\n",
    "tokenizer = SD3Tokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize_with_weights(prompts[0])\n",
    "print(tokens['l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping key 'shared.weight' in safetensors file as 'shared' does not exist in python model\n"
     ]
    }
   ],
   "source": [
    "from sd3.sd3_train import ClipL, ClipG, T5XXL\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_text_encoders(clipl_weights, clipg_weights, t5_weights):\n",
    "    clip_l = ClipL(clipl_weights)\n",
    "    clip_g = ClipG(clipg_weights)\n",
    "    t5 = T5XXL(t5_weights)\n",
    "    return clip_l, clip_g, t5\n",
    "\n",
    "clipl_weights = \"models/official/clip_l.safetensors\"\n",
    "clipg_weights = \"models/official/clip_g.safetensors\"\n",
    "t5_weights = \"models/official/t5xxl.safetensors\"\n",
    "clip_l, clip_g, t5 = load_text_encoders(clipl_weights, clipg_weights, t5_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 2])\n",
      "torch.Size([2, 77, 2])\n",
      "torch.Size([1, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "token_l = tokens[\"l\"].copy()\n",
    "token_l = torch.tensor(token_l)\n",
    "print(token_l.shape)\n",
    "# repeat token_l in the batch dimension\n",
    "token_l = token_l.repeat(2, 1, 1)\n",
    "print(token_l.shape)\n",
    "# token_l to list\n",
    "token_l = token_l.tolist()\n",
    "a = clip_l.model.encode_token_weights(token_l)\n",
    "print(a[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "a = clip_l.model.encode_token_weights(token_l)\n",
    "print(a[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_prompt_with_t5(\n",
    "    text_encoder,\n",
    "    tokenizer,\n",
    "    max_sequence_length,\n",
    "    prompt=None,\n",
    "    num_images_per_prompt=1,\n",
    "    device=None,\n",
    "    text_input_ids=None,\n",
    "):\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_sequence_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "    else:\n",
    "        if text_input_ids is None:\n",
    "            raise ValueError(\"text_input_ids must be provided when the tokenizer is not specified\")\n",
    "\n",
    "    prompt_embeds = text_encoder(text_input_ids.to(device))[0]\n",
    "\n",
    "    dtype = text_encoder.dtype\n",
    "    prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n",
    "\n",
    "    _, seq_len, _ = prompt_embeds.shape\n",
    "\n",
    "    # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    return prompt_embeds\n",
    "\n",
    "def _encode_prompt_with_clip(\n",
    "    text_encoder,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    device=None,\n",
    "    text_input_ids=None,\n",
    "    num_images_per_prompt: int = 1,\n",
    "):\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "    else:\n",
    "        if text_input_ids is None:\n",
    "            raise ValueError(\"text_input_ids must be provided when the tokenizer is not specified\")\n",
    "    \n",
    "    prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)\n",
    "\n",
    "    pooled_prompt_embeds = prompt_embeds[0]\n",
    "    prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "    prompt_embeds = prompt_embeds.to(dtype=text_encoder.dtype, device=device)\n",
    "\n",
    "    _, seq_len, _ = prompt_embeds.shape\n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "def encode_prompt(\n",
    "    text_encoders,\n",
    "    tokenizers,\n",
    "    prompt: str,\n",
    "    max_sequence_length,\n",
    "    device=None,\n",
    "    num_images_per_prompt: int = 1,\n",
    "    text_input_ids_list=None,\n",
    "):\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "\n",
    "    clip_tokenizers = tokenizers[:2]\n",
    "    clip_text_encoders = text_encoders[:2]\n",
    "\n",
    "    clip_prompt_embeds_list = []\n",
    "    clip_pooled_prompt_embeds_list = []\n",
    "    for i, (tokenizer, text_encoder) in enumerate(zip(clip_tokenizers, clip_text_encoders)):\n",
    "        prompt_embeds, pooled_prompt_embeds = _encode_prompt_with_clip(\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device if device is not None else next(text_encoder.model.parameters()).device,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            text_input_ids=text_input_ids_list[i] if text_input_ids_list else None,\n",
    "        )\n",
    "        clip_prompt_embeds_list.append(prompt_embeds)\n",
    "        clip_pooled_prompt_embeds_list.append(pooled_prompt_embeds)\n",
    "\n",
    "    clip_prompt_embeds = torch.cat(clip_prompt_embeds_list, dim=-1)\n",
    "    pooled_prompt_embeds = torch.cat(clip_pooled_prompt_embeds_list, dim=-1)\n",
    "\n",
    "    t5_prompt_embed = _encode_prompt_with_t5(\n",
    "        text_encoders[-1],\n",
    "        tokenizers[-1],\n",
    "        max_sequence_length,\n",
    "        prompt=prompt,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        text_input_ids=text_input_ids_list[-1] if text_input_ids_list else None,\n",
    "        device=device if device is not None else next(text_encoders[-1].model.parameters()).device, \n",
    "    )\n",
    "\n",
    "    clip_prompt_embeds = torch.nn.functional.pad(\n",
    "        clip_prompt_embeds, (0, t5_prompt_embed.shape[-1] - clip_prompt_embeds.shape[-1])\n",
    "    )\n",
    "    prompt_embeds = torch.cat([clip_prompt_embeds, t5_prompt_embed], dim=-2)\n",
    "\n",
    "    return prompt_embeds, pooled_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sequence_length = 77\n",
    "\n",
    "# prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "#     text_encoders=[clip_l, clip_g, t5],\n",
    "#     tokenizers=[None, None, None],\n",
    "#     prompt=prompts,\n",
    "#     max_sequence_length=max_sequence_length,\n",
    "#     text_input_ids_list=[tokens_clip_l, tokens_clip_g, tokens_t5],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD3Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_clip_l,\n",
    "        tokenizer_clip_g,\n",
    "        tokenizer_t5,\n",
    "        clip_l,\n",
    "        clip_g,\n",
    "        t5xxl,\n",
    "    ):\n",
    "        self.tokenizer_clip_l = tokenizer_clip_l\n",
    "        self.clip_l = clip_l\n",
    "        \n",
    "        self.tokenizer_clip_g = tokenizer_clip_g\n",
    "        self.clip_g = clip_g\n",
    "        \n",
    "        self.tokenizer_t5 = tokenizer_t5\n",
    "        self.t5xxl = t5xxl\n",
    "    \n",
    "    def get_cond(self, prompts):        \n",
    "        tokens = self.tokenize_with_weights(prompts)\n",
    "        \n",
    "        l_out, l_pooled = self.clip_l.model.encode_token_weights(tokens[\"l\"])\n",
    "        g_out, g_pooled = self.clip_g.model.encode_token_weights(tokens[\"g\"])\n",
    "        t5_out, t5_pooled = self.t5xxl.model.encode_token_weights(tokens[\"t5xxl\"])\n",
    "        \n",
    "        lg_out = torch.cat([l_out, g_out], dim=-1)\n",
    "        lg_out = torch.nn.functional.pad(lg_out, (0, 4096 - lg_out.shape[-1]))\n",
    "        # return prompt_embeds and pooled_prompt_embeds\n",
    "        return torch.cat([lg_out, t5_out], dim=-2), torch.cat(\n",
    "            (l_pooled, g_pooled), dim=-1\n",
    "        )\n",
    "    \n",
    "    def tokenize_with_weights(self, prompts):\n",
    "        tokens = {}\n",
    "        tokens['l'] = ...\n",
    "        tokens['g'] = ...\n",
    "        tokens['t5xxl'] = ...\n",
    "        return tokens\n",
    "\n",
    "sd3_trainer = SD3Trainer(tokenizer_clip_l, tokenizer_clip_g, tokenizer_t5, clip_l, clip_g, t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
